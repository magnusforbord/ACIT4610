{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n",
    "\n",
    "# Initialize Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.95  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "decay_rate = 0.995\n",
    "episodes = 10000\n",
    "\n",
    "# List to store total rewards for each episode\n",
    "rewards = []\n",
    "\n",
    "# Q-learning training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        action = np.random.choice(env.action_space.n) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
    "        \n",
    "        # Take the action and observe the outcome\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update Q-table using Q-learning formula\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    # Append total rewards for this episode to the rewards list\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * decay_rate)\n",
    "\n",
    "    # Print progress every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}: Total Rewards = {total_rewards}\")\n",
    "\n",
    "# Save Q-table to disk (optional)\n",
    "np.save(\"Q_table.npy\", Q)\n",
    "\n",
    "# Plot the total rewards over episodes\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.title('Q-learning Agent Performance Over Time')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Q-learning agent compared to heuristic and random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Ensure the results folder exists\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "# Initialize the Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Load the Q-learning agent (pre-trained Q-table)\n",
    "Q = np.load(\"Q_table.npy\")  # Make sure you saved this after training the agent\n",
    "\n",
    "# Define the heuristic-based policy\n",
    "def heuristic_policy(env, state):\n",
    "    \"\"\"A simple heuristic policy for Taxi-v3.\"\"\"\n",
    "    taxi_row, taxi_col, passenger, destination = env.decode(state)\n",
    "\n",
    "    # Locations of pickup and dropoff points in the 5x5 grid\n",
    "    locations = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
    "\n",
    "    # If the passenger is not in the taxi (passenger < 4), move towards the passenger\n",
    "    if passenger < 4:\n",
    "        passenger_row, passenger_col = locations[passenger]\n",
    "        if taxi_row < passenger_row:\n",
    "            return 1  # South\n",
    "        elif taxi_row > passenger_row:\n",
    "            return 0  # North\n",
    "        elif taxi_col < passenger_col:\n",
    "            return 2  # East\n",
    "        elif taxi_col > passenger_col:\n",
    "            return 3  # West\n",
    "        else:\n",
    "            return 4  # Pick-up\n",
    "    # If the passenger is in the taxi (passenger == 4), move towards the destination\n",
    "    else:\n",
    "        destination_row, destination_col = locations[destination]\n",
    "        if taxi_row < destination_row:\n",
    "            return 1  # South\n",
    "        elif taxi_row > destination_row:\n",
    "            return 0  # North\n",
    "        elif taxi_col < destination_col:\n",
    "            return 2  # East\n",
    "        elif taxi_col > destination_col:\n",
    "            return 3  # West\n",
    "        else:\n",
    "            return 5  # Drop-off\n",
    "\n",
    "# Evaluation function for Q-learning agent\n",
    "def test_q_learning_agent(env, Q, episodes=100):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_rewards = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])  # Choose best action using Q-table\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_rewards += reward\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "        steps.append(step_count)\n",
    "\n",
    "    return rewards, steps\n",
    "\n",
    "# Evaluation function for random policy\n",
    "def test_random_policy(env, episodes=100):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_rewards = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_rewards += reward\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "        steps.append(step_count)\n",
    "\n",
    "    return rewards, steps\n",
    "\n",
    "# Evaluation function for heuristic policy\n",
    "def test_heuristic_policy(env, episodes=100):\n",
    "    rewards = []\n",
    "    steps = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_rewards = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = heuristic_policy(env, state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_rewards += reward\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "        steps.append(step_count)\n",
    "\n",
    "    return rewards, steps\n",
    "\n",
    "# Number of episodes for evaluation\n",
    "episodes = 100\n",
    "\n",
    "# Test Q-learning agent\n",
    "q_learning_rewards, q_learning_steps = test_q_learning_agent(env, Q)\n",
    "\n",
    "# Test random policy\n",
    "random_rewards, random_steps = test_random_policy(env, episodes)\n",
    "\n",
    "# Test heuristic policy\n",
    "heuristic_rewards, heuristic_steps = test_heuristic_policy(env, episodes)\n",
    "\n",
    "# Plot and compare cumulative rewards\n",
    "plt.plot(q_learning_rewards, label='Q-learning Agent')\n",
    "plt.plot(random_rewards, label='Random Policy')\n",
    "plt.plot(heuristic_rewards, label='Heuristic Policy')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Cumulative Rewards')\n",
    "plt.title('Comparison of Agent, Random, and Heuristic Policies - Cumulative Rewards')\n",
    "plt.legend()\n",
    "plt.savefig('results/rewards_comparison.png')  # Save plot to results folder\n",
    "plt.show()\n",
    "\n",
    "# Plot and compare steps taken\n",
    "plt.plot(q_learning_steps, label='Q-learning Agent')\n",
    "plt.plot(random_steps, label='Random Policy')\n",
    "plt.plot(heuristic_steps, label='Heuristic Policy')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps Taken')\n",
    "plt.title('Comparison of Agent, Random, and Heuristic Policies - Steps Taken')\n",
    "plt.legend()\n",
    "plt.savefig('results/steps_comparison.png')  # Save plot to results folder\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization and comparison with other algorithms (SARSA & REINFORCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Load Taxi environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Suppress the deprecated warning from Gym\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# SARSA Algorithm (on-policy learning)\n",
    "def sarsa(env, alpha=0.1, gamma=0.95, epsilon=1.0, episodes=10000):\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    rewards = []\n",
    "    steps = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        action = np.random.choice(env.action_space.n) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_action = np.random.choice(env.action_space.n) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
    "\n",
    "            # SARSA update\n",
    "            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(0.01, epsilon * 0.99)\n",
    "\n",
    "        # Store total reward and steps for this episode\n",
    "        rewards.append(total_reward)\n",
    "        steps.append(step_count)\n",
    "\n",
    "        # Print progress every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"SARSA - Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Steps: {step_count}\")\n",
    "\n",
    "    return Q, rewards, steps\n",
    "\n",
    "# Q-Learning Algorithm (off-policy learning)\n",
    "def q_learning(env, alpha=0.1, gamma=0.99, epsilon=1.0, episodes=10000, epsilon_min=0.01, decay_rate=0.995):\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    rewards = []\n",
    "    steps = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action = np.random.choice(env.action_space.n) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "            state = next_state\n",
    "            total_rewards += reward\n",
    "            step_count += 1\n",
    "\n",
    "        rewards.append(total_rewards)\n",
    "        steps.append(step_count)\n",
    "        epsilon = max(epsilon_min, epsilon * decay_rate)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Q-learning - Episode {episode}: Total Rewards = {total_rewards}, Steps = {step_count}\")\n",
    "\n",
    "    return Q, rewards, steps\n",
    "\n",
    "# Define the Policy Network for REINFORCE\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.softmax(self.output(x), dim=-1)\n",
    "\n",
    "# Function to calculate discounted rewards with baseline subtraction\n",
    "def compute_discounted_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        cumulative_reward = reward + gamma * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)\n",
    "    return discounted_rewards - discounted_rewards.mean()\n",
    "\n",
    "# Reward shaping function\n",
    "def get_shaped_reward(env, state, action):\n",
    "    taxi_row, taxi_col, passenger, destination = env.decode(state)\n",
    "    locations = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
    "    target_row, target_col = locations[passenger if passenger < 4 else destination]\n",
    "    if action == 0:\n",
    "        taxi_row = min(taxi_row + 1, 4)\n",
    "    elif action == 1:\n",
    "        taxi_row = max(taxi_row - 1, 0)\n",
    "    elif action == 2:\n",
    "        taxi_col = min(taxi_col + 1, 4)\n",
    "    elif action == 3:\n",
    "        taxi_col = max(taxi_col - 1, 0)\n",
    "    old_distance = abs(taxi_row - target_row) + abs(taxi_col - target_col)\n",
    "    new_distance = abs(taxi_row - target_row) + abs(taxi_col - target_col)\n",
    "    return 0.1 if new_distance < old_distance else -0.1\n",
    "\n",
    "# REINFORCE algorithm with reward shaping\n",
    "def reinforce(env, policy, optimizer, gamma=0.99, episodes=1000):\n",
    "    total_rewards = []\n",
    "    steps_taken = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_rewards = []\n",
    "        episode_log_probs = []\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.eye(env.observation_space.n)[state]  # One-hot encoding of the state\n",
    "            action_probs = policy(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            log_prob = distribution.log_prob(action)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "\n",
    "            reward += get_shaped_reward(env, state, action.item())\n",
    "            episode_rewards.append(reward)\n",
    "            episode_log_probs.append(log_prob)\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "\n",
    "        discounted_rewards = compute_discounted_rewards(episode_rewards, gamma)\n",
    "        loss = -torch.sum(torch.stack([log_prob * reward for log_prob, reward in zip(episode_log_probs, discounted_rewards)]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_reward = sum(episode_rewards)\n",
    "        total_rewards.append(total_reward)\n",
    "        steps_taken.append(step_count)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"REINFORCE - Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Steps: {step_count}\")\n",
    "\n",
    "    return total_rewards, steps_taken\n",
    "\n",
    "# Initialize and train REINFORCE policy network\n",
    "policy_net = PolicyNetwork(env.observation_space.n, env.action_space.n)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "rewards_reinforce, steps_reinforce = reinforce(env, policy_net, optimizer, episodes=1000)\n",
    "\n",
    "# Train and compare SARSA and Q-learning\n",
    "Q_sarsa, rewards_sarsa, steps_sarsa = sarsa(env)\n",
    "Q_q_learning, rewards_q_learning, steps_q_learning = q_learning(env, episodes=10000)\n",
    "\n",
    "# Individual Plot for SARSA\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(rewards_sarsa)), rewards_sarsa, label='SARSA - Rewards', color='b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('SARSA - Total Rewards Over Episodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('results/SARSA_rewards.png')  # Save plot to results folder\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(steps_sarsa)), steps_sarsa, label='SARSA - Steps', color='b')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps Taken')\n",
    "plt.title('SARSA - Steps Taken Over Episodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('results/SARSA_steps.png')  # Save plot to results folder\n",
    "plt.show()\n",
    "\n",
    "# Individual Plot for REINFORCE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(rewards_reinforce)), rewards_reinforce, label='REINFORCE - Rewards', color='g')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('REINFORCE - Total Rewards Over Episodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('results/REINFORCE_rewards.png')  # Save plot to results folder\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(steps_reinforce)), steps_reinforce, label='REINFORCE - Steps', color='g')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps Taken')\n",
    "plt.title('REINFORCE - Steps Taken Over Episodes')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('results/REINFROCE_steps.png')  # Save plot to results folder\n",
    "plt.show()\n",
    "\n",
    "# Plot comparison of rewards for SARSA, Q-learning, and REINFORCE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(rewards_sarsa)), rewards_sarsa, label='SARSA - Rewards', color='b', linestyle='-', linewidth=1.5)\n",
    "plt.plot(range(len(rewards_q_learning)), rewards_q_learning, label='Q-learning - Rewards', color='r', linestyle='--', linewidth=1.5)\n",
    "plt.plot(range(len(rewards_reinforce)), rewards_reinforce, label='REINFORCE - Rewards', color='g', linestyle='-.', linewidth=1.5)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('SARSA vs Q-learning vs REINFORCE - Total Rewards')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('results/Q_SARSA_REINFROCE_rewards.png')  # Save plot to results folder\n",
    "plt.show()\n",
    "\n",
    "# Plot comparison of steps taken for SARSA, Q-learning, and REINFORCE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(steps_sarsa)), steps_sarsa, label='SARSA - Steps', color='b', linestyle='-', linewidth=1.5)\n",
    "plt.plot(range(len(steps_q_learning)), steps_q_learning, label='Q-learning - Steps', color='r', linestyle='--', linewidth=1.5)\n",
    "plt.plot(range(len(steps_reinforce)), steps_reinforce, label='REINFORCE - Steps', color='g', linestyle='-.', linewidth=1.5)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Steps Taken')\n",
    "plt.title('SARSA vs Q-learning vs REINFORCE - Steps Taken')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('results/Q_SARSA_REINFORCE_steps.png')  # Save plot to results folder\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
